<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Network Architectures - Evolution of AI | Electi Consulting</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica', 'Arial', sans-serif;
            background: linear-gradient(135deg, #fafafa 0%, #f0f0f0 100%);
            color: #2a2a2a;
            overflow: hidden;
            line-height: 1.7;
        }

        .presentation-container {
            width: 100vw;
            height: 100vh;
            position: relative;
            overflow: hidden;
        }

        .slide {
            width: 100%;
            height: 100%;
            position: absolute;
            top: 0;
            left: 0;
            display: none;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 40px;
            opacity: 0;
            transition: opacity 0.5s ease-in-out;
        }

        .slide.active {
            display: flex;
            opacity: 1;
        }

        .slide-content {
            max-width: 1100px;
            width: 100%;
            max-height: 85vh;
            overflow-y: auto;
            padding: 30px 40px;
        }

        /* Custom scrollbar */
        .slide-content::-webkit-scrollbar {
            width: 8px;
        }

        .slide-content::-webkit-scrollbar-track {
            background: rgba(139, 28, 52, 0.08);
            border-radius: 4px;
        }

        .slide-content::-webkit-scrollbar-thumb {
            background: #8B1C34;
            border-radius: 4px;
        }

        .slide-content::-webkit-scrollbar-thumb:hover {
            background: #a52138;
        }

        /* Title Slide */
        .title-slide h1 {
            font-size: clamp(2.5rem, 5vw, 4.5rem);
            color: #8B1C34;
            margin-bottom: 25px;
            text-align: center;
            font-weight: 800;
            letter-spacing: -1.5px;
            line-height: 1.2;
        }

        .title-slide h2 {
            font-size: clamp(1.3rem, 3vw, 2.2rem);
            color: #2a2a2a;
            margin-bottom: 40px;
            text-align: center;
            font-weight: 400;
            line-height: 1.5;
        }

        .title-slide .subtitle {
            font-size: clamp(1rem, 2vw, 1.4rem);
            color: #555555;
            text-align: center;
            margin-top: 40px;
            line-height: 1.8;
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
        }

        .electi-logo {
            font-size: clamp(1.6rem, 2.5vw, 2.2rem);
            color: #8B1C34;
            font-weight: 800;
            letter-spacing: 4px;
            margin-top: 60px;
            text-align: center;
            text-shadow: 0 2px 4px rgba(139, 28, 52, 0.15);
        }

        /* Content Slides */
        .content-slide h2 {
            font-size: clamp(2rem, 4vw, 3.2rem);
            color: #8B1C34;
            margin-bottom: 35px;
            border-bottom: 4px solid #8B1C34;
            padding-bottom: 18px;
            font-weight: 700;
            letter-spacing: -0.5px;
            line-height: 1.3;
        }

        .content-slide h3 {
            font-size: clamp(1.4rem, 2.5vw, 2.1rem);
            color: #8B1C34;
            margin-top: 35px;
            margin-bottom: 18px;
            font-weight: 600;
            line-height: 1.4;
        }

        .architecture-card {
            background: rgba(139, 28, 52, 0.04);
            border-left: 5px solid #8B1C34;
            padding: 25px 30px;
            margin-bottom: 30px;
            border-radius: 10px;
            transition: transform 0.3s ease, background 0.3s ease, box-shadow 0.3s ease;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        }

        .architecture-card:hover {
            transform: translateX(8px);
            background: rgba(139, 28, 52, 0.08);
            box-shadow: 0 4px 16px rgba(139, 28, 52, 0.2);
        }

        .architecture-card h4 {
            color: #8B1C34;
            font-size: clamp(1.2rem, 2vw, 1.6rem);
            margin-bottom: 18px;
            font-weight: 700;
            letter-spacing: -0.3px;
        }

        .architecture-detail {
            margin-bottom: 15px;
            line-height: 1.8;
            font-size: clamp(0.95rem, 1.5vw, 1.15rem);
            color: #3a3a3a;
        }

        .architecture-detail strong {
            color: #8B1C34;
            font-weight: 700;
            letter-spacing: 0.3px;
        }

        .references {
            margin-top: 20px;
            padding-top: 20px;
            border-top: 2px solid rgba(139, 28, 52, 0.2);
        }

        .references ol {
            margin-left: 25px;
            font-size: clamp(0.85rem, 1.3vw, 1rem);
            color: #555555;
            line-height: 1.7;
        }

        .references li {
            margin-bottom: 12px;
            line-height: 1.7;
        }

        .references strong {
            color: #8B1C34;
            font-weight: 600;
        }

        /* Timeline Slides */
        .timeline-item {
            position: relative;
            padding-left: 45px;
            margin-bottom: 35px;
            border-left: 3px solid #8B1C34;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: -9px;
            top: 0;
            width: 16px;
            height: 16px;
            border-radius: 50%;
            background: #8B1C34;
            border: 4px solid #fafafa;
            box-shadow: 0 0 0 2px rgba(139, 28, 52, 0.2);
        }

        .timeline-year {
            color: #8B1C34;
            font-size: clamp(1.3rem, 2vw, 1.9rem);
            font-weight: 800;
            margin-bottom: 12px;
            letter-spacing: -0.5px;
        }

        .timeline-content {
            color: #3a3a3a;
            font-size: clamp(0.95rem, 1.5vw, 1.15rem);
            line-height: 1.8;
        }

        .timeline-content strong {
            color: #8B1C34;
            font-weight: 700;
        }

        /* Navigation */
        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 20px;
            z-index: 1000;
            background: rgba(255, 255, 255, 0.98);
            padding: 18px 30px;
            border-radius: 50px;
            backdrop-filter: blur(10px);
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);
            border: 1px solid rgba(139, 28, 52, 0.1);
        }

        .nav-button {
            background: #8B1C34;
            color: #ffffff;
            border: none;
            padding: 14px 28px;
            font-size: clamp(0.95rem, 1.5vw, 1.05rem);
            cursor: pointer;
            border-radius: 30px;
            transition: all 0.3s ease;
            font-weight: 700;
            letter-spacing: 0.5px;
            box-shadow: 0 2px 8px rgba(139, 28, 52, 0.25);
        }

        .nav-button:hover:not(:disabled) {
            background: #a52138;
            transform: scale(1.05);
            box-shadow: 0 4px 12px rgba(139, 28, 52, 0.4);
        }

        .nav-button:disabled {
            background: rgba(139, 28, 52, 0.3);
            cursor: not-allowed;
            box-shadow: none;
        }

        .slide-counter {
            color: #2a2a2a;
            display: flex;
            align-items: center;
            font-size: clamp(0.95rem, 1.5vw, 1.05rem);
            font-weight: 600;
            padding: 0 20px;
            letter-spacing: 0.5px;
        }

        /* Progress Bar */
        .progress-bar {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 4px;
            background: rgba(139, 28, 52, 0.1);
            z-index: 1001;
        }

        .progress-fill {
            height: 100%;
            background: #8B1C34;
            transition: width 0.3s ease;
            box-shadow: 0 0 10px rgba(139, 28, 52, 0.3);
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .slide {
                padding: 20px 15px;
            }

            .slide-content {
                padding: 20px 15px;
            }

            .architecture-card {
                padding: 20px;
            }

            .timeline-item {
                padding-left: 30px;
            }

            .navigation {
                bottom: 15px;
                padding: 12px 18px;
                gap: 12px;
            }

            .nav-button {
                padding: 12px 20px;
                font-size: 0.9rem;
            }

            .references ol {
                margin-left: 18px;
            }

            .overview-grid {
                grid-template-columns: 1fr;
                gap: 20px;
            }

            .slide-counter {
                padding: 0 12px;
                font-size: 0.9rem;
            }
        }

        @media (max-width: 480px) {
            .slide-content {
                padding: 15px 10px;
            }
            
            .architecture-card {
                padding: 15px;
                margin-bottom: 20px;
            }

            .content-slide h2 {
                margin-bottom: 25px;
                padding-bottom: 12px;
            }

            .navigation {
                padding: 10px 15px;
            }

            .nav-button {
                padding: 10px 16px;
                font-size: 0.85rem;
            }
        }

        /* Overview Grid */
        .overview-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin-top: 30px;
        }

        .overview-card {
            background: rgba(139, 28, 52, 0.04);
            padding: 25px;
            border-radius: 10px;
            border: 2px solid rgba(139, 28, 52, 0.15);
            transition: all 0.3s ease;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        }

        .overview-card:hover {
            border-color: #8B1C34;
            background: rgba(139, 28, 52, 0.08);
            transform: translateY(-5px);
            box-shadow: 0 6px 20px rgba(139, 28, 52, 0.2);
        }

        .overview-card h3 {
            color: #8B1C34;
            font-size: clamp(1.1rem, 1.8vw, 1.4rem);
            margin-bottom: 15px;
            font-weight: 700;
            line-height: 1.4;
        }

        .overview-card p {
            color: #3a3a3a;
            font-size: clamp(0.9rem, 1.3vw, 1.05rem);
            line-height: 1.7;
        }

        /* Key Innovation Highlight */
        .key-innovation {
            background: linear-gradient(135deg, rgba(139, 28, 52, 0.08) 0%, rgba(139, 28, 52, 0.03) 100%);
            padding: 18px 20px;
            border-radius: 10px;
            margin: 18px 0;
            border-left: 5px solid #8B1C34;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
            line-height: 1.8;
        }

        .key-innovation strong {
            color: #8B1C34;
            font-weight: 700;
            font-size: 1.05em;
        }
    </style>
</head>
<body>
    <div class="progress-bar">
        <div class="progress-fill" id="progressFill"></div>
    </div>

    <div class="presentation-container">
        <!-- Slide 1: Title -->
        <div class="slide active title-slide">
            <div class="slide-content">
                <h1>Neural Network Architectures</h1>
                <h2>The Evolution of Artificial Intelligence</h2>
                <p class="subtitle">A Comprehensive Journey Through 67 Years of Innovation<br>1958 - 2025</p>
                <div class="electi-logo">ELECTI CONSULTING</div>
            </div>
        </div>

        <!-- Slide 2: Overview -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>The Journey of Neural Networks</h2>
                <div class="overview-grid">
                    <div class="overview-card">
                        <h3>Early Foundations</h3>
                        <p>1958-1989: From single perceptrons to multi-layer architectures that could learn complex patterns</p>
                    </div>
                    <div class="overview-card">
                        <h3>The Deep Learning Revolution</h3>
                        <p>1990-2014: Breakthroughs in recurrent networks, convolutional architectures, and unsupervised learning</p>
                    </div>
                    <div class="overview-card">
                        <h3>Modern AI Era</h3>
                        <p>2014-2020: Attention mechanisms, transformers, and architectures that power today's AI systems</p>
                    </div>
                    <div class="overview-card">
                        <h3>Frontier Research</h3>
                        <p>2020-2025: Efficient architectures, memory systems, and brain-inspired reasoning models</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 3: Early Era (1958-1989) -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>The Foundations (1958-1989)</h2>
                
                <div class="architecture-card">
                    <h4>1. Perceptron (1958)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> First trainable neural network model with threshold activation
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Binary classification, pattern recognition</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Simplicity, online learning capability</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Cannot solve linearly non-separable problems (e.g., XOR)</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Single-layer perceptron, Mark I Perceptron hardware</div>
                    <div class="references">
                        <strong>Key References:</strong>
                        <ol>
                            <li>Rosenblatt, F. (1958). "The perceptron: A probabilistic model for information storage and organization in the brain." Psychological Review, 65(6), 386-408.</li>
                            <li>Rosenblatt, F. (1961). "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms." Spartan Books.</li>
                            <li>Minsky, M., & Papert, S. (1969). "Perceptrons: An Introduction to Computational Geometry." MIT Press.</li>
                        </ol>
                    </div>
                </div>

                <div class="architecture-card">
                    <h4>2. Hopfield Networks (1982)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Energy-based recurrent network with binary units
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Associative memory, pattern completion, optimization</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Content-addressable memory, pattern recovery, local minima convergence</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Limited storage capacity, susceptible to spurious memories</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Modern Hopfield Networks with continuous states</div>
                </div>
            </div>
        </div>

        <!-- Slide 4: 1982-1987 -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Building Complexity (1982-1987)</h2>
                
                <div class="architecture-card">
                    <h4>3. Self-Organizing Maps (SOMs) (1982)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Competitive learning for dimensionality reduction and topological mapping
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Visualization, clustering, topology preservation, data exploration</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Unsupervised learning, preserves topological properties, intuitive visualization</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Fixed structure once trained, sensitive to initialization</div>
                </div>

                <div class="architecture-card">
                    <h4>4. Boltzmann Machines (1986)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Stochastic binary units with symmetric connections using energy-based learning
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Unsupervised learning, probabilistic modeling, feature extraction</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Learn complex probability distributions, model uncertainty, energy-based formulation</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Computationally expensive training, difficult to scale</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Restricted Boltzmann Machines, Deep Boltzmann Machines</div>
                </div>
            </div>
        </div>

        <!-- Slide 5: MLP and RBFN -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>The Learning Revolution (1986-1987)</h2>
                
                <div class="architecture-card">
                    <h4>5. Multilayer Perceptron (MLP) (1986)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Multiple layers of fully-connected neurons with nonlinear activation functions and backpropagation training
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Classification, regression, feature learning, tabular data analysis, pattern recognition</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Universal function approximation, conceptual simplicity, versatility, effective with structured data</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Prone to overfitting, sensitive to initialization, difficulty with very deep architectures</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Feedforward networks, Deep MLPs, Wide & Deep networks</div>
                    <div class="references">
                        <strong>Key References:</strong>
                        <ol>
                            <li>Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). "Learning representations by back-propagating errors." Nature, 323(6088), 533-536.</li>
                            <li>Cybenko, G. (1989). "Approximation by superpositions of a sigmoidal function." Mathematics of Control, Signals, and Systems, 2(4), 303-314.</li>
                        </ol>
                    </div>
                </div>

                <div class="architecture-card">
                    <h4>6. Radial Basis Function Networks (RBFNs) (1987)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Hidden units with radial basis function activation for localized responses
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Function approximation, time series prediction, interpolation, control systems</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Universal approximation capability, faster training for some problems, local specialization</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Curse of dimensionality, requires many units for complex functions</div>
                </div>
            </div>
        </div>

        <!-- Slide 6: RNNs and CNNs -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Sequential and Spatial Processing (1990-1998)</h2>
                
                <div class="architecture-card">
                    <h4>7. Recurrent Neural Networks (RNNs) (1990)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Hidden state that carries information across time steps enabling temporal processing
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Sequential data processing, time series analysis, NLP, speech recognition</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Variable length input/output, parameter sharing across time, temporal context awareness</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Vanishing/exploding gradients, difficulty capturing long-range dependencies</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Elman networks, Jordan networks, Bidirectional RNNs</div>
                </div>

                <div class="architecture-card">
                    <h4>8. Convolutional Neural Networks (CNNs) (1998)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Hierarchical feature learning through local receptive fields and weight sharing
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, object detection, computer vision, pattern recognition</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Translation invariance, parameter efficiency, hierarchical feature learning, spatial locality</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Requires large datasets, computationally intensive, limited to grid-like data</div>
                    <div class="architecture-detail"><strong>Examples:</strong> LeNet-5, AlexNet, VGGNet, ResNet</div>
                </div>
            </div>
        </div>

        <!-- Slide 7: LSTM and Autoencoders -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Memory and Representation (1997-2006)</h2>
                
                <div class="architecture-card">
                    <h4>9. Long Short-Term Memory (LSTM) (1997)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Gated memory cells to address vanishing gradient problem in RNNs
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Long sequence modeling, machine translation, speech recognition, time series</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Captures long-range dependencies, selective information retention, stable gradient flow</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Computational complexity, memory requirements, slow training</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Bidirectional LSTM, Stacked LSTM, Peephole LSTM</div>
                </div>

                <div class="architecture-card">
                    <h4>10. Autoencoders (2006)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Unsupervised learning through reconstruction with dimensionality reduction bottleneck
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Dimensionality reduction, feature learning, denoising, anomaly detection</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Unsupervised learning, compact representations, feature extraction, noise tolerance</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Can learn trivial solutions, sensitive to architecture design</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Denoising Autoencoders, Variational Autoencoders (VAEs), Sparse Autoencoders</div>
                </div>
            </div>
        </div>

        <!-- Slide 8: Deep Architectures -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>The Deep Learning Era (2012-2014)</h2>
                
                <div class="architecture-card">
                    <h4>11. AlexNet (2012)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Deep CNN trained on GPUs with ReLU activation, dropout, and data augmentation
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Large-scale image classification, object recognition, visual features</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> GPU acceleration, effective regularization, breakthrough performance on ImageNet</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Required large datasets, computationally expensive</div>
                </div>

                <div class="architecture-card">
                    <h4>12. Variational Autoencoders (VAEs) (2013)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Probabilistic generative models with continuous latent variables
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Generative modeling, latent space interpolation, semi-supervised learning</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Principled probabilistic framework, smooth latent space, controllable generation</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Blurry generations, complexity of ELBO optimization</div>
                </div>

                <div class="architecture-card">
                    <h4>13. Generative Adversarial Networks (GANs) (2014)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Two-network adversarial training for realistic data generation
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image generation, style transfer, data augmentation, super-resolution</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> High-quality generation, no need for explicit likelihood, versatile applications</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Training instability, mode collapse, difficult to evaluate</div>
                </div>
            </div>
        </div>

        <!-- Slide 9: Gated Networks -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Efficient Sequential Models (2014-2015)</h2>
                
                <div class="architecture-card">
                    <h4>14. Gated Recurrent Units (GRUs) (2014)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Simplified gated architecture with fewer parameters than LSTM
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Sequence modeling, machine translation, speech recognition</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Fewer parameters than LSTM, faster training, comparable performance</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> May underperform LSTM on some tasks, less flexible gating</div>
                </div>

                <div class="architecture-card">
                    <h4>15. VGGNet (2014)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Very deep CNNs with small 3Ã—3 filters throughout
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, feature extraction, transfer learning</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Simple architecture, small filters, strong performance, good feature representations</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Computationally expensive, many parameters, memory intensive</div>
                </div>

                <div class="architecture-card">
                    <h4>16. Inception Networks (GoogleNet) (2014)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Multi-scale feature extraction through parallel convolutions
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, object detection, efficient deep learning</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Efficient multi-scale feature extraction, reduced parameters, computational efficiency</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Complex architecture, difficult to modify, intricate design choices</div>
                </div>
            </div>
        </div>

        <!-- Slide 10: ResNets and Beyond -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Enabling Very Deep Networks (2015-2016)</h2>
                
                <div class="architecture-card">
                    <h4>17. Residual Networks (ResNets) (2015)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Skip connections enabling training of very deep networks (100+ layers)
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, object detection, feature extraction</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Enables very deep networks, easier optimization, better gradient flow</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Increased memory usage, some redundancy in representations</div>
                    <div class="architecture-detail"><strong>Examples:</strong> ResNet-50, ResNet-101, ResNet-152, Wide ResNets</div>
                </div>

                <div class="architecture-card">
                    <h4>18. Sequence-to-Sequence Models (Seq2Seq) (2015)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Encoder-decoder architecture with attention for variable-length sequence mapping
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Machine translation, summarization, question answering, dialogue systems</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Handles variable-length I/O, end-to-end learning, attention mechanism</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Information bottleneck in fixed-size context vector</div>
                </div>

                <div class="architecture-card">
                    <h4>19. U-Net (2015)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Encoder-decoder with skip connections for dense prediction tasks
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image segmentation, medical imaging, dense prediction</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Preserves fine-grained details, works with limited data, symmetric architecture</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Memory intensive, requires pixel-level annotations</div>
                </div>
            </div>
        </div>

        <!-- Slide 11: 2016 Architectures -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Architectural Innovations (2016)</h2>
                
                <div class="architecture-card">
                    <h4>20. DenseNet (2016)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Dense connectivity where each layer connects to all subsequent layers
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, object detection, feature reuse</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Feature reuse, parameter efficiency, strong gradient flow</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> High memory consumption, complex implementation</div>
                </div>

                <div class="architecture-card">
                    <h4>21. Neural Architecture Search (NAS) (2016)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Automated discovery of neural network architectures using RL or evolution
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Architecture discovery, model design automation, AutoML</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Discovers novel architectures, task-specific optimization, reduces human bias</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Extremely computationally expensive, requires significant resources</div>
                </div>

                <div class="architecture-card">
                    <h4>22. Capsule Networks (2016)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Groups of neurons (capsules) representing instantiation parameters of entities
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, pose estimation, viewpoint handling</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Better handling of spatial relationships, viewpoint equivariance</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Computationally expensive, difficult to scale, limited adoption</div>
                </div>
            </div>
        </div>

        <!-- Slide 12: The Transformer Revolution -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>The Transformer Revolution (2017)</h2>
                
                <div class="architecture-card">
                    <h4>23. Transformer (2017)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Self-attention mechanism eliminating recurrence for parallelizable sequence processing
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Machine translation, language modeling, NLP tasks, vision, audio</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Parallelizable training, captures long-range dependencies, scalable, versatile</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Quadratic complexity with sequence length, data hungry</div>
                    <div class="architecture-detail"><strong>Examples:</strong> BERT, GPT, T5, ViT, DALL-E</div>
                    <div class="references">
                        <strong>Key References:</strong>
                        <ol>
                            <li>Vaswani, A., et al. (2017). "Attention is all you need." Advances in Neural Information Processing Systems, 30.</li>
                            <li>Devlin, J., et al. (2018). "BERT: Pre-training of deep bidirectional transformers for language understanding." NAACL-HLT.</li>
                            <li>Brown, T. B., et al. (2020). "Language models are few-shot learners." NeurIPS.</li>
                        </ol>
                    </div>
                </div>

                <div class="architecture-card">
                    <h4>24. MobileNets (2017)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Depthwise separable convolutions for mobile-efficient architectures
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Mobile vision, edge computing, resource-constrained deployment</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Lightweight, fast inference, low latency, mobile-friendly</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Accuracy tradeoff, limited capacity</div>
                </div>
            </div>
        </div>

        <!-- Slide 13: Neural ODE and Modern Architectures -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Continuous and Efficient Architectures (2018-2019)</h2>
                
                <div class="architecture-card">
                    <h4>25. Neural Ordinary Differential Equations (Neural ODEs) (2018)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Continuous-depth models using ODE solvers for forward pass
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Time series, continuous normalizing flows, irregular sampling</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Constant memory cost, adaptive computation, continuous representations</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Slow training, numerical stability concerns</div>
                </div>

                <div class="architecture-card">
                    <h4>26. Graph Neural Networks (GNNs) (2018)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Neural networks operating directly on graph-structured data
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Social networks, molecular property prediction, knowledge graphs</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Handles non-Euclidean data, permutation invariance, relational reasoning</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Over-smoothing, scalability challenges</div>
                    <div class="architecture-detail"><strong>Examples:</strong> GCN, GraphSAGE, GAT, GIN</div>
                </div>

                <div class="architecture-card">
                    <h4>27. EfficientNet (2019)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Compound scaling method balancing depth, width, and resolution
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, transfer learning, efficient deep learning</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> State-of-the-art accuracy with fewer parameters, systematic scaling</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Complex scaling rules, architecture-specific</div>
                </div>
            </div>
        </div>

        <!-- Slide 14: Language Models -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Foundation Language Models (2018-2019)</h2>
                
                <div class="architecture-card">
                    <h4>28. BERT (2018)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Bidirectional pre-training with masked language modeling
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Text classification, question answering, NLU tasks</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Bidirectional context, transfer learning, strong performance</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Not suitable for generation, requires fine-tuning</div>
                </div>

                <div class="architecture-card">
                    <h4>29. GPT-2 (2019)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Large-scale autoregressive language model with zero-shot capabilities
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Text generation, language understanding, few-shot learning</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Zero-shot generalization, coherent long-form generation</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Unidirectional context, potential for harmful generation</div>
                </div>

                <div class="architecture-card">
                    <h4>30. T5 (2019)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Unified text-to-text framework for all NLP tasks
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> All NLP tasks as text generation problems</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Unified framework, transfer learning, versatility</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Large model size, computational requirements</div>
                </div>
            </div>
        </div>

        <!-- Slide 15: Large Language Models Era -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Large Language Models (2020)</h2>
                
                <div class="architecture-card">
                    <h4>31. GPT-3 (2020)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Massive scale (175B parameters) enabling few-shot learning without fine-tuning
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Language generation, translation, reasoning, code generation</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Few-shot learning, remarkable versatility, emergent capabilities</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Enormous computational cost, potential biases, unpredictable behavior</div>
                    <div class="architecture-detail"><strong>Examples:</strong> ChatGPT, Codex, InstructGPT</div>
                </div>

                <div class="architecture-card">
                    <h4>32. Perceiver (2020)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Cross-attention to learned latent array, handling arbitrary input modalities
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Multimodal learning, high-dimensional inputs, general-purpose modeling</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Modality-agnostic, scalable to large inputs, flexible architecture</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Requires careful latent design, complex training</div>
                </div>
            </div>
        </div>

        <!-- Slide 16: Mixture of Experts -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Scaling Through Sparsity (2021-2022)</h2>
                
                <div class="architecture-card">
                    <h4>33. Mixture of Experts (MoE) Transformers (2021)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Sparse activation where routing mechanism selects subset of expert networks
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Large-scale language modeling, multimodal learning, efficient scaling</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Efficient scaling to trillions of parameters, specialized experts</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Complex training, load balancing challenges, inference complexity</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Switch Transformer, GLaM, Sparse Transformers</div>
                </div>

                <div class="architecture-card">
                    <h4>34. Diffusion Models (2021)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Iterative denoising process for high-quality generation
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image generation, audio synthesis, video generation, inpainting</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> High-quality samples, stable training, mode coverage</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Slow generation (many steps), computational cost</div>
                    <div class="architecture-detail"><strong>Examples:</strong> DALL-E 2, Stable Diffusion, Imagen, Midjourney</div>
                </div>

                <div class="architecture-card">
                    <h4>35. PaLM (2022)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> 540B parameter decoder-only model with breakthrough reasoning capabilities
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Complex reasoning, multilingual tasks, code understanding</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Strong reasoning, efficient architecture, multilingual performance</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Massive scale required, computational expense</div>
                </div>
            </div>
        </div>

        <!-- Slide 17: Recent Architectures -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Modern Innovations (2022-2023)</h2>
                
                <div class="architecture-card">
                    <h4>36. Chinchilla (2022)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Optimal compute allocation between model size and training tokens
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Efficient language modeling, compute-optimal training</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Better performance per compute, data-efficient scaling laws</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Requires more training data, longer training</div>
                </div>

                <div class="architecture-card">
                    <h4>37. InstructGPT / RLHF Models (2022)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Reinforcement learning from human feedback for alignment
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Conversational AI, instruction following, helpful assistants</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Aligned with human preferences, safer outputs, instruction following</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Complex training pipeline, potential reward hacking</div>
                </div>

                <div class="architecture-card">
                    <h4>38. LLaMA (2023)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Open-source foundation models competitive with proprietary alternatives
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Open research, custom fine-tuning, democratized AI</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Open access, research transparency, strong performance</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Requires careful fine-tuning, potential misuse concerns</div>
                </div>
            </div>
        </div>

        <!-- Slide 18: Vision and Multimodal -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Vision Transformers (2020-2023)</h2>
                
                <div class="architecture-card">
                    <h4>39. Vision Transformers (ViT) (2020)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Pure transformer architecture applied to image patches
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Image classification, object detection, segmentation, visual recognition</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Global receptive field, scalability, adaptability, transfer learning capability</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Data hunger, lack of inductive biases, computational requirements</div>
                    <div class="architecture-detail"><strong>Examples:</strong> ViT, DeiT, Swin Transformer, BEiT, MAE</div>
                </div>

                <div class="architecture-card">
                    <h4>40. Mamba / Selective State Space Models (2023)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Selective State Space Models with hardware-aware parallel scan algorithm
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Long-sequence modeling, genomics, audio processing, language modeling</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Linear time scaling (O(N)), constant memory inference, higher throughput than Transformers</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Difficulty with copying tasks, different inductive bias than attention</div>
                    <div class="architecture-detail"><strong>Examples:</strong> Mamba-3B, Mamba-2, Jamba (Hybrid), Vim (Vision Mamba)</div>
                </div>
            </div>
        </div>

        <!-- Slide 19: Cutting Edge 2024-2025 -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Frontier Research (2024-2025)</h2>
                
                <div class="architecture-card">
                    <h4>41. Kolmogorov-Arnold Networks (KANs) (2024)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Replace weight-based networks with learned univariate functions
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Scientific ML, function approximation, interpretable AI, physics-informed learning</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Higher expressivity with fewer parameters, interpretability, data efficiency</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Computational complexity, training instability, scaling challenges</div>
                </div>

                <div class="architecture-card">
                    <h4>42. Titans (2024)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Neural Memory module that learns to memorize at test time
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Infinite context processing, needle-in-haystack, meta-learning, time series</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Scales to millions of tokens, O(1) access to history, dynamic weight updates</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Complexity of training memory module, novel paradigm</div>
                </div>
            </div>
        </div>

        <!-- Slide 20: Latest Innovations -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>The Next Generation (2025)</h2>
                
                <div class="architecture-card">
                    <h4>43. Nested Learning (2025)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Unifying architecture and optimization into nested loops to solve catastrophic forgetting
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Continual learning, lifelong learning agents, real-time knowledge updating</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Prevents catastrophic forgetting, allows internal knowledge updates, mirrors neuroplasticity</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> High complexity in tuning, currently proof-of-concept stage</div>
                </div>

                <div class="architecture-card">
                    <h4>44. Hierarchical Reasoning Models (HRM) (2025)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Brain-inspired dual-loop architecture with separate planning and execution modules
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Complex reasoning (Sudoku, mazes), logical inference, ARC-AGI, system-2 reasoning</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Extreme parameter efficiency (<30M beats LLMs), data efficiency, inference-time scaling</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Complex nested optimization, requires deep supervision, slower inference</div>
                </div>

                <div class="architecture-card">
                    <h4>45. Tiny Recurrent Models (TRM) (2025)</h4>
                    <div class="key-innovation">
                        <strong>Key Innovation:</strong> Minimalist recursive architecture using single small network (~7M parameters)
                    </div>
                    <div class="architecture-detail"><strong>Application Areas:</strong> Data-scarce reasoning, embedded logic solving, flexible generalization, ARC-AGI</div>
                    <div class="architecture-detail"><strong>Advantages:</strong> Superior generalization vs larger models, prevents overfitting, self-correction capability</div>
                    <div class="architecture-detail"><strong>Limitations:</strong> Lacks explicit planning/execution separation, many iteration steps needed</div>
                </div>
            </div>
        </div>

        <!-- Slide 21: Timeline Overview -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Evolution Timeline: Key Milestones</h2>
                
                <div class="timeline-item">
                    <div class="timeline-year">1958 - 1989</div>
                    <div class="timeline-content">
                        <strong>Foundation Era:</strong> From single perceptrons to multilayer networks with backpropagation. The groundwork for modern neural networks.
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-year">1990 - 2005</div>
                    <div class="timeline-content">
                        <strong>Specialized Architectures:</strong> RNNs for sequences, CNNs for vision, LSTM for long-term dependencies. Neural networks tackle domain-specific challenges.
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-year">2006 - 2011</div>
                    <div class="timeline-content">
                        <strong>Deep Learning Emergence:</strong> Autoencoders, deep belief networks, and unsupervised pre-training revive neural network research.
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-year">2012 - 2016</div>
                    <div class="timeline-content">
                        <strong>Deep Learning Revolution:</strong> AlexNet, VGG, ResNet, GANs. Deep learning achieves superhuman performance on vision tasks.
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-year">2017 - 2019</div>
                    <div class="timeline-content">
                        <strong>Transformer Era Begins:</strong> Self-attention replaces recurrence. BERT, GPT-2, and efficient architectures transform NLP and beyond.
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-year">2020 - 2023</div>
                    <div class="timeline-content">
                        <strong>Foundation Models:</strong> GPT-3, PaLM, LLaMA, and diffusion models. Massive scale enables emergent capabilities and few-shot learning.
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-year">2024 - 2025</div>
                    <div class="timeline-content">
                        <strong>Next-Gen Architectures:</strong> Mamba, KANs, Titans, Nested Learning, HRM. Focus shifts to efficiency, reasoning, and continual learning.
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 22: Key Trends and Patterns -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Architectural Trends Across Eras</h2>
                
                <div class="overview-grid">
                    <div class="overview-card">
                        <h3>Scale & Efficiency</h3>
                        <p>Models grew from thousands to trillions of parameters. Recent focus on efficiency: Mamba's linear scaling, KANs' parameter reduction, TRMs' minimalism.</p>
                    </div>

                    <div class="overview-card">
                        <h3>Memory & Context</h3>
                        <p>From LSTMs (limited memory) â†’ Transformers (attention) â†’ Titans (test-time memory) â†’ Nested Learning (continual updates).</p>
                    </div>

                    <div class="overview-card">
                        <h3>Specialization â†’ Generalization</h3>
                        <p>Domain-specific architectures (CNNs, RNNs) gave way to general-purpose models (Transformers, Perceivers) that handle multiple modalities.</p>
                    </div>

                    <div class="overview-card">
                        <h3>Reasoning Capabilities</h3>
                        <p>From pattern recognition to emergent reasoning. Latest: HRMs and TRMs tackle complex logical tasks with minimal parameters.</p>
                    </div>

                    <div class="overview-card">
                        <h3>Biological Inspiration</h3>
                        <p>Recurring theme: Hopfield Networks, Capsule Networks, HRMs mirror cortical hierarchies. Brain-inspired architectures making comeback.</p>
                    </div>

                    <div class="overview-card">
                        <h3>Training Paradigms</h3>
                        <p>Supervised â†’ Self-supervised â†’ Few-shot â†’ Zero-shot â†’ Test-time learning. Each era reduces data and supervision requirements.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 23: Application Domains -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>Transformative Applications</h2>
                
                <div class="architecture-card">
                    <h4>Natural Language Processing</h4>
                    <div class="architecture-detail">
                        <strong>Evolution:</strong> RNNs â†’ LSTMs â†’ Transformers â†’ GPT-3 â†’ ChatGPT<br>
                        <strong>Impact:</strong> Machine translation, content generation, conversational AI, code generation
                    </div>
                </div>

                <div class="architecture-card">
                    <h4>Computer Vision</h4>
                    <div class="architecture-detail">
                        <strong>Evolution:</strong> CNNs â†’ ResNets â†’ Vision Transformers â†’ Diffusion Models<br>
                        <strong>Impact:</strong> Image classification, object detection, image generation, medical imaging
                    </div>
                </div>

                <div class="architecture-card">
                    <h4>Scientific Computing</h4>
                    <div class="architecture-detail">
                        <strong>Evolution:</strong> MLPs â†’ Neural ODEs â†’ Physics-Informed NNs â†’ KANs<br>
                        <strong>Impact:</strong> Protein folding, drug discovery, climate modeling, materials science
                    </div>
                </div>

                <div class="architecture-card">
                    <h4>Autonomous Systems</h4>
                    <div class="architecture-detail">
                        <strong>Evolution:</strong> RNNs â†’ Seq2Seq â†’ Transformers â†’ Multimodal Models<br>
                        <strong>Impact:</strong> Self-driving cars, robotics, game AI, decision-making systems
                    </div>
                </div>

                <div class="architecture-card">
                    <h4>Creative AI</h4>
                    <div class="architecture-detail">
                        <strong>Evolution:</strong> GANs â†’ VAEs â†’ Diffusion Models<br>
                        <strong>Impact:</strong> Art generation, music composition, video synthesis, design tools
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 24: Future Directions -->
        <div class="slide content-slide">
            <div class="slide-content">
                <h2>The Road Ahead</h2>
                
                <div class="architecture-card">
                    <h4>Emerging Research Directions</h4>
                    
                    <div class="architecture-detail">
                        <strong>Continual Learning:</strong> Neural architectures that learn continuously without forgetting, like Nested Learning systems that update internal knowledge in real-time.
                    </div>

                    <div class="architecture-detail">
                        <strong>Efficient Scaling:</strong> Moving beyond pure parameter scaling to architectures like Mamba that achieve linear complexity, or TRMs that maximize performance with minimal parameters.
                    </div>

                    <div class="architecture-detail">
                        <strong>Interpretability:</strong> KANs and other architectures that provide transparency into decision-making processes, crucial for scientific and high-stakes applications.
                    </div>

                    <div class="architecture-detail">
                        <strong>Multimodal Integration:</strong> Unified models that seamlessly process text, images, audio, video, and sensor data without modality-specific preprocessing.
                    </div>

                    <div class="architecture-detail">
                        <strong>Reasoning & Planning:</strong> HRMs and similar architectures that separate high-level planning from low-level execution, mirroring human cognitive processes.
                    </div>

                    <div class="architecture-detail">
                        <strong>Test-Time Adaptation:</strong> Titans and related approaches that update model behavior dynamically during inference, adapting to new contexts without retraining.
                    </div>

                    <div class="architecture-detail">
                        <strong>Neuromorphic Computing:</strong> Brain-inspired architectures designed for neuromorphic hardware, enabling energy-efficient AI at the edge.
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 25: Conclusion -->
        <div class="slide title-slide">
            <div class="slide-content">
                <h1>67 Years of Innovation</h1>
                <h2>From Single Neurons to Intelligent Systems</h2>
                <p class="subtitle">
                    The evolution of neural networks represents one of humanity's most remarkable technological journeys.<br><br>
                    From Rosenblatt's perceptron in 1958 to today's sophisticated reasoning models,<br>
                    each architecture has built upon previous innovations,<br>
                    creating the AI revolution we witness today.
                </p>
                <div class="electi-logo">ELECTI CONSULTING</div>
                <p class="subtitle" style="margin-top: 30px; font-size: 1rem;">
                    Leading the Future of AI Implementation
                </p>
            </div>
        </div>

    </div>

    <!-- Navigation Controls -->
    <div class="navigation">
        <button class="nav-button" id="prevBtn" onclick="changeSlide(-1)">Previous</button>
        <span class="slide-counter">
            <span id="currentSlide">1</span> / <span id="totalSlides">25</span>
        </span>
        <button class="nav-button" id="nextBtn" onclick="changeSlide(1)">Next</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');
        const currentSlideSpan = document.getElementById('currentSlide');
        const totalSlidesSpan = document.getElementById('totalSlides');
        const progressFill = document.getElementById('progressFill');

        // Initialize
        totalSlidesSpan.textContent = totalSlides;
        updateSlide();

        function changeSlide(direction) {
            slides[currentSlide].classList.remove('active');
            currentSlide += direction;
            
            if (currentSlide >= totalSlides) {
                currentSlide = 0;
            } else if (currentSlide < 0) {
                currentSlide = totalSlides - 1;
            }
            
            updateSlide();
        }

        function updateSlide() {
            slides[currentSlide].classList.add('active');
            currentSlideSpan.textContent = currentSlide + 1;
            
            // Update progress bar
            const progress = ((currentSlide + 1) / totalSlides) * 100;
            progressFill.style.width = progress + '%';
            
            // Update button states
            prevBtn.disabled = false;
            nextBtn.disabled = false;
            
            // Scroll to top of slide content
            const slideContent = slides[currentSlide].querySelector('.slide-content');
            if (slideContent) {
                slideContent.scrollTop = 0;
            }
        }

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowLeft') {
                changeSlide(-1);
            } else if (e.key === 'ArrowRight') {
                changeSlide(1);
            }
        });

        // Touch/swipe support for mobile
        let touchStartX = 0;
        let touchEndX = 0;

        document.addEventListener('touchstart', (e) => {
            touchStartX = e.changedTouches[0].screenX;
        });

        document.addEventListener('touchend', (e) => {
            touchEndX = e.changedTouches[0].screenX;
            handleSwipe();
        });

        function handleSwipe() {
            if (touchEndX < touchStartX - 50) {
                changeSlide(1); // Swipe left
            }
            if (touchEndX > touchStartX + 50) {
                changeSlide(-1); // Swipe right
            }
        }

        // Prevent text selection during swipes
        document.addEventListener('selectstart', (e) => {
            if (e.target.tagName !== 'INPUT' && e.target.tagName !== 'TEXTAREA') {
                e.preventDefault();
            }
        });
    </script>
</body>
</html>